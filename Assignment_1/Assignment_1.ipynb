{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "__Note__: *For general instructions about Assignment 1 as well as deadlines, please refer to the acompanying PDF document on Studium.*\n",
    "\n",
    "### Setup\n",
    "\n",
    "This notebook is there to help you complete Assignment 1. It contains 3 sections (one for each task of the assignment) that guide you through the processes of completing the assignment. First import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import re\n",
    "from ipywebrtc import CameraStream, VideoRecorder\n",
    "from src.app import AnnotationApp\n",
    "from src.landmark_names import landmark_names\n",
    "import imageio as iio\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "import matplotlib.pyplot as plt\n",
    "from src.skbot_replacement import linear_trajectory\n",
    "from zipfile import ZipFile\n",
    "from IPython.display import YouTubeVideo\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gestures = [\n",
    "    \"ASL_letter_A\",\n",
    "    \"ASL_letter_B\",\n",
    "    \"ASL_letter_C\",\n",
    "    \"ASL_letter_L\",\n",
    "    \"ASL_letter_R\",\n",
    "    \"ASL_letter_U\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "\n",
    "This notebook will generate the files you have to upload in completion of Task 1 and Task 2. In order to ensure that these files will be named correctly, update the variables below with the appropriate values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the variables below to the correct values\n",
    "last_name = \"Norinder\"\n",
    "first_name = \"Niklas\"\n",
    "student_number = \"4686\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Anonymized Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recording the Gestures\n",
    "\n",
    "Task 1 asks you to record a total of 6 videos of your hand (6 seconds per video), with each video showing a different gesture. The required gestures change from year to year, so please refer to the instruction document for Assignment 1 for the exact gestures that you will have to record.\n",
    "\n",
    "Currently, the overarching theme is american sign language (ASL) and below you can find a visualization of the ASL alphabet:\n",
    "\n",
    "![ASL Alphabet](https://t3.ftcdn.net/jpg/02/42/65/88/360_F_242658839_l38u5LVWSROrFMLQasxLJgl6sk8yAq9m.jpg)\n",
    "(Source: Adobe Stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** While recording the videos move your hand around slowly. For this, pick a constant direction (in 3D) and move your hand slowly in that direction. This is done to create more diverse training data. The hand can be anywhere on the screen, arbitrarily rotated, and at any distance from the camera (as long as it is still clearly identifiable as a hand). The broader the set of examples, the more robust the trained model will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/VSu_Nkn7TzY\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x141c20be0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo(\"VSu_Nkn7TzY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Example 1_: Place your hand on the left side of the image, show the gesture and start recording. Then - over the course of 6 seconds - slowly move your hand to the right side of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/ZEdfzmMntHs\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x141c20250>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo(\"ZEdfzmMntHs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Example 2_: Place your hand in the bottom of the image, show the gesture and start recording. Then - over the course of 6 seconds - move your hand up towards the top of.\n",
    "\n",
    "*__Note__: We will aggregate the videos from all assignments into a large dataset that you will use during the final project. For this you will anonymize the recorded videos in the next step by blurring your face. If you feel that this level of anonymization is insufficient, you can avoid recording your face or use another means of anonymization.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a47a3690b74e89873fd21f55bd60f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CameraStream(constraints={'facing_mode': 'user', 'audio': False, 'video': {'width': 640, 'height': 480}})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "camera = CameraStream(constraints=\n",
    "                      {'facing_mode': 'user',\n",
    "                       'audio': False,\n",
    "                       'video': { 'width': 640, 'height': 480}\n",
    "                       })\n",
    "camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baecf8765cfa4610b0c5be3981a80689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VideoRecorder(stream=CameraStream(constraints={'facing_mode': 'user', 'audio': False, 'video': {'width': 640, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recorder = VideoRecorder(stream=camera)\n",
    "recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are happy with the video save it as a MP4 file. The filename should be the name of the gesture that has been recorded. For example, if you recorded the letter A, then the filename would be `ASL_letter_A.mp4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture_name = \"ASL_letter_R\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Could not load meta information\n=== stderr ===\n\nffmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers\n  built with Apple clang version 11.0.0 (clang-1100.0.33.8)\n  configuration: --enable-gpl --enable-version3 --enable-sdl2 --enable-fontconfig --enable-gnutls --enable-iconv --enable-libass --enable-libdav1d --enable-libbluray --enable-libfreetype --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libopus --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libtheora --enable-libtwolame --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libzimg --enable-lzma --enable-zlib --enable-gmp --enable-libvidstab --enable-libvorbis --enable-libvo-amrwbenc --enable-libmysofa --enable-libspeex --enable-libxvid --enable-libaom --enable-appkit --enable-avfoundation --enable-coreimage --enable-audiotoolbox\n  libavutil      56. 31.100 / 56. 31.100\n  libavcodec     58. 54.100 / 58. 54.100\n  libavformat    58. 29.100 / 58. 29.100\n  libavdevice    58.  8.100 / 58.  8.100\n  libavfilter     7. 57.100 /  7. 57.100\n  libswscale      5.  5.100 /  5.  5.100\n  libswresample   3.  5.100 /  3.  5.100\n  libpostproc    55.  5.100 / 55.  5.100\n/var/folders/cf/pw1h2ybx4y51r2y9kbm86g040000gn/T/imageio_23r5z9rl: Invalid data found when processing input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cf/pw1h2ybx4y51r2y9kbm86g040000gn/T/ipykernel_79218/1238212248.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0miio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ffmpeg\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvideo_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/imageio/core/imopen.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, index, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mim\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/imageio/core/imopen.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mim\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/imageio/core/imopen.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \"\"\"\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/imageio/core/imopen.py\u001b[0m in \u001b[0;36mlegacy_get_reader\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/imageio/core/format.py\u001b[0m in \u001b[0;36mget_reader\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0;34mf\"Format {self.name} cannot read in {request.mode.image_mode} mode\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             )\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/imageio/core/format.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, format, request)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;31m# Open the reader/writer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/imageio/plugins/ffmpeg.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, loop, size, dtype, pixelformat, print_info, ffmpeg_params, input_params, output_params, fps)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;31m# Start ffmpeg subprocess and get meta information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;31m# For cameras, create thread that keeps reading the images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/imageio/plugins/ffmpeg.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    474\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# we already have meta data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/imageio_ffmpeg/_io.py\u001b[0m in \u001b[0;36mread_frames\u001b[0;34m(path, pix_fmt, bpp, input_params, output_params, bits_per_pixel)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0merr2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_catcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mfmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Could not load meta information\\n=== stderr ===\\n{}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m\"No such file or directory\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlog_catcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} not found! Wrong path?\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Could not load meta information\n=== stderr ===\n\nffmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers\n  built with Apple clang version 11.0.0 (clang-1100.0.33.8)\n  configuration: --enable-gpl --enable-version3 --enable-sdl2 --enable-fontconfig --enable-gnutls --enable-iconv --enable-libass --enable-libdav1d --enable-libbluray --enable-libfreetype --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libopus --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libtheora --enable-libtwolame --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libzimg --enable-lzma --enable-zlib --enable-gmp --enable-libvidstab --enable-libvorbis --enable-libvo-amrwbenc --enable-libmysofa --enable-libspeex --enable-libxvid --enable-libaom --enable-appkit --enable-avfoundation --enable-coreimage --enable-audiotoolbox\n  libavutil      56. 31.100 / 56. 31.100\n  libavcodec     58. 54.100 / 58. 54.100\n  libavformat    58. 29.100 / 58. 29.100\n  libavdevice    58.  8.100 / 58.  8.100\n  libavfilter     7. 57.100 /  7. 57.100\n  libswscale      5.  5.100 /  5.  5.100\n  libswresample   3.  5.100 /  3.  5.100\n  libpostproc    55.  5.100 / 55.  5.100\n/var/folders/cf/pw1h2ybx4y51r2y9kbm86g040000gn/T/imageio_23r5z9rl: Invalid data found when processing input"
     ]
    }
   ],
   "source": [
    "with iio.imopen(recorder.video.value, \"r\", format=\"ffmpeg\") as video_in:\n",
    "    frames = video_in.read()\n",
    "    print(type(video_in.read()))\n",
    "     \n",
    "    \n",
    "w = iio.get_writer(f\"{gesture_name}.mp4\", format='FFMPEG', mode='I', fps=30)\n",
    "for frame in frames:\n",
    "    w.append_data(frame)\n",
    "\n",
    "w.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please review the videos before you continue** and ensure that:\n",
    "\n",
    "-\tThe name of the file must match the gesture shown in the video\n",
    "-\tThe video must be in MP4 format\n",
    "-\tThe gesture is shown for at least 6 seconds\n",
    "-\tThe hand showing the gesture is not occluded by any objects\n",
    "-\tThe hand showing the gesture does not occlude your face (if present)\n",
    "-\tThe video must be anonymized\n",
    "\n",
    "If your video does not satisfy the above, __we will reject it__ and you will have to record it again (and redo the annotations for that video)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anonymizing the Video\n",
    "\n",
    "Run the snippet below. It uses the face detection method presented in lab 1 to track your face and then applies gaussian blur to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for condition in gestures:\n",
    "    frames = iio.imread(f\"{condition}.mp4\")\n",
    "    face_cascade = cv2.CascadeClassifier('frontal_face_features.xml')\n",
    "\n",
    "    # find the face in each frame (only keep exact matches)\n",
    "    timestamps = list()\n",
    "    face_coords = list()\n",
    "    for idx, frame in enumerate(frames):\n",
    "        faces = face_cascade.detectMultiScale(frame)\n",
    "        if len(faces) == 1:\n",
    "            timestamps.append(idx)\n",
    "            face_coords.append(faces[0])\n",
    "            \n",
    "    if timestamps == 0:\n",
    "        # no faces detected\n",
    "        iio.imwrite(f\"{condition}_anonymous.mp4\", frames, fps=30)\n",
    "        continue\n",
    "    \n",
    "    # Track the faces across frames and smooth the track\n",
    "    if timestamps[0] != 0:\n",
    "        timestamps.insert(0, 0)\n",
    "        face_coords.insert(0, face_coords[0])\n",
    "    if timestamps[-1] != len(frames):\n",
    "        timestamps.append(len(frames))\n",
    "        face_coords.append(face_coords[-1])\n",
    "    t_control = np.stack(timestamps)\n",
    "    face_coords = np.stack(face_coords)  \n",
    "    face_coords_interpolated = linear_trajectory(np.arange(len(frames)), face_coords, t_control=t_control)\n",
    "    face_coords_interpolated = np.round(face_coords_interpolated).astype(int)\n",
    "    \n",
    "    old_shape = face_coords_interpolated.shape\n",
    "    shape = (old_shape[0] - 5, old_shape[1], 5)\n",
    "    old_strides = face_coords_interpolated.strides\n",
    "    strides = (*old_strides, old_strides[0])    \n",
    "    face_coords_smoothed = np.round(as_strided(face_coords_interpolated, shape, strides).mean(-1)).astype(int)\n",
    "\n",
    "    # blurr the tracked area\n",
    "    # 1. extract the face ROI in each frame\n",
    "    # 2. apply incrementally apply mild blurs to incrementally larger areas\n",
    "    #    (this gives the final result a smoother look)\n",
    "    for frame, pos in zip(frames, face_coords_smoothed): \n",
    "        for partition in range(3, 10):\n",
    "            inner = pos.copy()\n",
    "            inner[0] = pos[0] + 1/partition * pos[2]\n",
    "            inner[1] = pos[1] + 1/partition * pos[3]\n",
    "            inner[2] = (partition-2)/partition * pos[2]\n",
    "            inner[3] = (partition-2)/partition * pos[3]\n",
    "\n",
    "            (x, y, w, h) = inner\n",
    "            roi = frame[y:y+h, x:x+w]   \n",
    "            roi = cv2.GaussianBlur(roi,(21, 21), 50)\n",
    "            frame[y:y+h, x:x+w] = roi\n",
    "\n",
    "        x, y, w, h = pos\n",
    "        roi = frame[y:y+h, x:x+w]   \n",
    "        roi = cv2.GaussianBlur(roi,(11, 11), 50)\n",
    "        frame[y:y+h, x:x+w] = roi\n",
    "\n",
    "    # write the frames to disk\n",
    "    num_frames = len(face_coords_smoothed)\n",
    "    iio.imwrite(f\"{condition}_anonymous.mp4\", frames[:num_frames], fps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the anonymized videos and make sure that you are happy with the result of the anonymization. Also check that the gesture is still clearly visible and not affected by the annonymization. If not, please re-record the video in question and repeat the anonymization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Task 1 for submission\n",
    "\n",
    "After you have generated all the videos, group them into a zip file that can be uploaded to Studium. For this, make sure you have updated the values of the variables defined in the **Metadata** section above. Then, run the snippet below to generate the ZIP file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile(f\"{last_name}_{first_name}_{student_number}_videos.zip\", \"w\") as zipy:\n",
    "    for condition in gestures:\n",
    "        zipy.write(f\"{condition}_anonymous.mp4\", f\"{condition}.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Annotating the Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotate the videos that you have created in Task 1. This task will show you some of the challenges associated with \"real world\" ML projects and it tends to be the most disliked step in a ML project. It is, however, one of the most crutial ones, and the better the quality of your annotations is, the better your resulting model tends to be. To help you along in this process, we provide you with an annotation tool.\n",
    "\n",
    "**Important:** This step is crucial for good performance of your machine learning model. ML is not black magic, and your model can only ever be as good as the data you use for training. If you are sloppy here, you will not have high performance later. The saying \"trash in, trash out\" is very true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to annotate the gestures:\n",
    "\n",
    "*__Note__: If the annotation is lacking in accuracy, we will ask you to re-annotate the video before you can pass assignment 1.*\n",
    "\n",
    "0. Only annotate <u>visible</u> joints.\n",
    "1. Go to the first frame of the gesture.\n",
    "2. Place the respective joint markers onto each joint according to the figure on the right.\n",
    "3. Go to the last frame of the gesture.\n",
    "4. Move the respective joint markers to their correct position in the last frame.\n",
    "5. Go back to the first frame of the gesture.\n",
    "6. Slowly move the \"Frames\" slider forward. Whenever you see that a marker deviates from the hand's joint, adjust it so that it follows the hand again.\n",
    "7. After all videos are annotated, click the \"Save CSV\" button in the top left corner to create the CSV containing the annotations. It will be placed next to the notebook.\n",
    "\n",
    "#### How to use this annotation tool:\n",
    "\n",
    "1. Use the \"Video\" drop-down menu to select and jump between gestures (progress is preserved).\n",
    "2. Move the \"Frames\" slider at the bottom to select individual frames (progress is preserved).\n",
    "3. Click on the figure on the right, use the blue arrows on the bottom, or use the \"Joint\" drop-down menu to select a joint to annotate.\n",
    "4. Click on the image on the left to place the respective joint marker. This will automatically jump to the next joint.\n",
    "5. The marker matching the currently selected joint will turn red and you can drag it around in the image.\n",
    "5. Use the blue buttons with label \"Keyframe\" to jump to the next (or previous) frame that had the current joint marker placed on it explicitly.\n",
    "6. Use the red \"🗑️ Keyframe\" button to delete the annotation of the current joint at the current frame. This will have no effect if the current frame is not a keyframe.\n",
    "7. Use the red \"Reset Joint\" button to delete all keyframes for the current joint.\n",
    "\n",
    "#### Example Video of an Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"9UYYc_L3E34\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Annotation Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "gui = AnnotationApp(gestures, f\"{last_name}_{first_name}_{student_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb91915fd2a048fda4ceff3b2f22763e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Button(button_style='success', description='Save CSV', icon='save', style=Button…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Sheet and Consent Form\n",
    "\n",
    "Please review (and sign) the __Information Sheet and Consent Form__ document that was distributed with this assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Annotated Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the annotations (CSV from task 2) on top of each gesture (video from task 1). For this, draw circles onto each frame at the position of each visible joint. Then, draw lines that connect all pairs of visible joints that are adjacent according to the skeleton. This should result in a skeleton drawn onto the frame similar to the skeleton you saw while annotating the videos. Finally, add the text \"annotated\" into the top left corner of the frame and save the frames of each gesture as a new video.\n",
    "\n",
    "Here is the program/algorithm in pseudo-code:\n",
    "\n",
    "```\n",
    "for each video in gestures:\n",
    "    for each frame in video:\n",
    "        place a label into the top left corner that reads \"annotated\"\n",
    "        get the corresponding joint positions from the CSV file\n",
    "        draw a circle at each (non-missing) joint position\n",
    "        for each pair of visible joints connected by the skeleton:\n",
    "            draw a line between the two joints using cv2.line\n",
    "    create a new_video file called <gesture>_annotated from the annotated frames\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv('Norinder_Niklas_4686.csv')\n",
    "\n",
    "for gesture in gestures:\n",
    "    with iio.imopen(f\"{gesture}.mp4\", \"r\", format=\"ffmpeg\") as video_file:\n",
    "        frames = video_file.read()\n",
    "        stacked_video = []\n",
    "        for j in range(len(frames)):\n",
    "            i = 0\n",
    "            k = 0\n",
    "            stack_arr = []\n",
    "            while i < len(csv['gesture']):\n",
    "                if csv['gesture'][i] == gesture:\n",
    "                    frame = cv2.circle(frames[j], (int(csv['x'][i]),int(csv['y'][i])), radius=3, color=(0, 0, 255), thickness=-1)\n",
    "                    cv2.putText(frame, \"annotated\", (0, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 6)\n",
    "                    cv2.putText(frame, \"annotated\", (0, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)   \n",
    "                    stack_arr.append(frame)\n",
    "                i = i + len(frames)\n",
    "\n",
    "            frame_stacked = np.concatenate(stack_arr, axis=1)\n",
    "            stacked_video.append(frame_stacked)\n",
    "    \n",
    "    stacked_video = np.stack(stacked_video)\n",
    "        \n",
    "    w = iio.get_writer(f\"{gesture}_annotated_test.mp4\", format='FFMPEG', mode='I', fps=30)\n",
    "    for vid_frame in stacked_video:\n",
    "        w.append_data(vid_frame)\n",
    "    w.close()\n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are done, combine the annotated videos into a ZIP file using the snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ASL_letter_A_annotated.mp4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cf/pw1h2ybx4y51r2y9kbm86g040000gn/T/ipykernel_74777/1887490093.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{last_name}_{first_name}_{student_number}_annotated.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzipy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcondition\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgestures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mzipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{condition}_annotated.mp4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/zipfile.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, filename, arcname, compress_type, compresslevel)\u001b[0m\n\u001b[1;32m   1725\u001b[0m             )\n\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1727\u001b[0;31m         zinfo = ZipInfo.from_file(filename, arcname,\n\u001b[0m\u001b[1;32m   1728\u001b[0m                                   strict_timestamps=self._strict_timestamps)\n\u001b[1;32m   1729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/zipfile.py\u001b[0m in \u001b[0;36mfrom_file\u001b[0;34m(cls, filename, arcname, strict_timestamps)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m         \u001b[0misdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS_ISDIR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0mmtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocaltime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ASL_letter_A_annotated.mp4'"
     ]
    }
   ],
   "source": [
    "with ZipFile(f\"{last_name}_{first_name}_{student_number}_annotated.zip\", \"w\") as zipy:\n",
    "    for condition in gestures:\n",
    "        zipy.write(f\"{condition}_annotated.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Studium\n",
    "\n",
    "Once you are done, upload the following files to Studium:\n",
    "\n",
    "1. The filled out and signed *Information Sheet and Consent Form*\n",
    "2. The ZIP file with the anonymous videos (`<last name>_<first name>_<student number>_videos.zip`)\n",
    "3. The CSV file containing the annotations (`<last name>_<first name>_<student number>.csv`)\n",
    "4. the ZIP file with the annotated videos (`<last name>_<first name>_<student number>_videos_annotated.zip`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
